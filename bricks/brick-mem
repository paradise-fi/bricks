// -*- mode: C++; indent-tabs-mode: nil; c-basic-offset: 4 -*-

/*
 * (c) 2015 Petr Roƒçkai <code@fixp.eu>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#pragma once

#include <vector>
#include <memory>
#include <map>
#include <atomic>
#include <tuple>

#ifndef NVALGRIND
#include <iostream>
#include <iomanip>
#endif

#ifndef NVALGRIND
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wold-style-cast"
#include <memcheck.h>
#pragma GCC diagnostic pop
#endif

#include <brick-types>
#include <brick-string>
#include <brick-hash>
#include <brick-shmem>
#include <brick-mmap>
#include <brick-bitlevel>

namespace brick {

namespace mem {

constexpr inline int align( int v, int a ) {
    return (v % a) ? (v + a - (v % a)) : v;
}

template< int i > struct _bitvec { using T = typename _bitvec< i + 1 >::T; };
template<> struct _bitvec<  8 > { using T = uint8_t; };
template<> struct _bitvec< 16 > { using T = uint16_t; };
template<> struct _bitvec< 32 > { using T = uint32_t; };
template<> struct _bitvec< 64 > { using T = uint64_t; };

template< int i > using bitvec = typename _bitvec< i >::T;

/*
 * A pool keeps track of memory in a compact, fast, thread-optimised fashion.
 * It is organised into slabs of chunks of a single size. The Pointer type
 * can be cheaply converted into an actual pointer or to the size of the object
 * it points to. Both pointers and their dereferences are stable (no object
 * moving happens). Freelists are inline and used in LIFO order, to minimise
 * cache turnaround. Excess free memory is linked into a global freelist which
 * is used when the thread-local lists and partial blocks run out.
 *
 * A single item is limited to 2^24 bytes (16M). Total memory use is capped at
 * roughly 16T (more if you use big objects), but can be easily extended. If
 * compiled in debug mode, (without -DNVALGRIND), destroying a pool will give
 * you some usage statistics. During runtime, valgrind will be kept up to date
 * about memory use and accessibility.
 *
 * The pointers are always 3 pieces, the slab, the chunk and the tag. The
 * layout is customisable (so that the tag bits, which are reserved for the
 * Pool's clients, can be positioned as needed within the pointer).
 */
template< int bits1 = 20, int bits2 = 19,
          typename RawT = bitvec< bits1 + bits2 >,
          int slab_position = 0, int chunk_position = 1, int tag_position = 2 >
struct Pool
{
    static_assert( tag_position + slab_position + chunk_position == 3, "bad pointer layout" );
    static_assert( tag_position >= 0 && slab_position >= 0 && chunk_position >= 0,
                   "negative positions" );
    static_assert( tag_position != slab_position && tag_position != chunk_position &&
                   slab_position != chunk_position, "position re-use" );
    static const int _bits3 = sizeof( RawT ) * 8 - bits1 - bits2;
    static_assert( _bits3 >= 0, "the raw type is too small" );

    struct Pointer : brick::types::Comparable
    {
        using Raw = RawT;
        using Bits = bitlevel::BitTuple<
            bitlevel::BitField< Raw, bits1 >,
            bitlevel::BitField< Raw, bits2 >,
            bitlevel::BitField< Raw, _bits3 > >;
        Bits _v;

        auto slab() { return bitlevel::get< slab_position >( _v ); }
        auto chunk() { return bitlevel::get< chunk_position >( _v ); }
        auto tag() { return bitlevel::get< tag_position >( _v ); }

        explicit Pointer( Raw s = 0, Raw c = 0 ) noexcept
        {
            slab() = s;
            chunk() = c;
        }

        Raw raw() const { return *reinterpret_cast< const Raw * >( _v.storage ); }
        Raw &raw() { return *reinterpret_cast< Raw * >( _v.storage ); }
        explicit operator bool() const { return raw(); }
        bool operator!() const { return !raw(); }
        bool operator<= ( Pointer o ) const { return _v < o._v || _v == o._v; }
    };

    static const int slab_bits = Pointer::Bits::template AccessAt< slab_position >::T::width;
    static const int chunk_bits = Pointer::Bits::template AccessAt< chunk_position >::T::width;

    struct BlockHeader
    {
        uint64_t total:20;
        uint64_t allocated:20;
        uint64_t itemsize:24;
        char data[0];
    };

    struct FreeList
    {
        Pointer head;
        FreeList *next;
        int32_t count;
        FreeList() : next( nullptr ), count( 0 ) {}
    };

    struct SizeInfo
    {
        int active, blocksize;
        FreeList touse, tofree;
        int perm_active, perm_blocksize;
        SizeInfo() : active( -1 ), blocksize( 4096 ), perm_active( -1 ) {}
        ~SizeInfo() {}
    };

    static void nukeList( FreeList *f )
    {
        while ( f ) {
            auto d = f;
            f = f->next;
            delete d;
        }
    }

    static const int blockcount = 1 << slab_bits;
    static const int blocksize  = 4 << chunk_bits;

    using FreeListPtr = std::atomic< FreeList * >;

    struct VHandle
    {
        int handle;
        bool allocated;
        VHandle() : handle( -1 ), allocated( false ) {}
    };

    struct Shared
    {
        BlockHeader *block[ blockcount ];
        std::atomic< int > usedblocks;
        FreeListPtr freelist[ 4096 ];
        std::atomic< FreeListPtr * > freelist_big[ 4096 ];
#ifndef NVALGRIND
        std::atomic< VHandle * > vhandles[ blockcount ]; /* one for each block */
#endif
    };

    struct Local
    {
        std::vector< int > emptyblocks;
        SizeInfo size[ 4096 ];
        SizeInfo *size_big[ 4096 ];
        int ephemeral_block;
        int ephemeral_offset;
    } _l;

    const std::shared_ptr< Shared > _s;

#ifndef NVALGRIND
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wold-style-cast"

    void valgrindInit()
    {
        for ( int i = 0; i < blockcount; ++i )
            _s->vhandles[ i ] = nullptr;
    }

    void valgrindAllocated( Pointer p, bool materialized )
    {
        VALGRIND_MEMPOOL_ALLOC( _s->block[ p.slab() ], dereference( p ), size( p ) );
        VALGRIND_MAKE_MEM_UNDEFINED( dereference( p ), size( p ) );

        VHandle *h = _s->vhandles[ p.slab() ], *alloc;
        if ( !h ) {
            if ( _s->vhandles[ p.slab() ].compare_exchange_strong(
                     h, alloc = new VHandle[ header( p ).total ]) )
                h = alloc;
            else
                delete[] alloc;
        }

        ASSERT( h );
        ASSERT( materialized || !h[ p.chunk() ].allocated );
        VALGRIND_DISCARD( h[ p.chunk() ].handle );
        h[ p.chunk() ].handle =
            VALGRIND_CREATE_BLOCK( dereference( p ), size( p ),
                                   brick::string::fmtf( "blob %llu:%llu @ %p",
                                                        p.slab(), p.chunk(),
                                                        dereference( p ) ).c_str() );
        h[ p.chunk() ].allocated = true;
    }

    void valgrindDeallocated( Pointer p )
    {
        VALGRIND_MEMPOOL_FREE( _s->block[ p.slab() ], dereference( p ) );
        VALGRIND_MAKE_MEM_NOACCESS( dereference( p ), size( p ) );

        ASSERT( _s->vhandles[ p.slab() ].load() );
        ASSERT( _s->vhandles[ p.slab() ][ p.chunk() ].allocated );

        VALGRIND_DISCARD( _s->vhandles[ p.slab() ][ p.chunk() ].handle );
        _s->vhandles[ p.slab() ][ p.chunk() ].handle =
            VALGRIND_CREATE_BLOCK( dereference( p ), size( p ),
                                   brick::string::fmtf( "blob %llu:%llu @ %p [DELETED]",
                                                        p.slab(), p.chunk(),
                                                        dereference( p ) ).c_str() );
        _s->vhandles[ p.slab() ][ p.chunk() ].allocated = false;
    }

    void valgrindNewBlock( int b, int bytes )
    {
        VALGRIND_MAKE_MEM_NOACCESS( _s->block[ b ] + sizeof( BlockHeader ), bytes );
        VALGRIND_CREATE_MEMPOOL( _s->block[ b ], 0, 0 );
    }

    void bump( std::vector< int64_t > &v, size_t cnt )
    {
        v.resize( std::max( v.size(), cnt + 1 ), 0 );
    }

    void valgrindFini()
    {
        int64_t count = 0;
        int64_t bytes = 0;
        int64_t wasted = 0;
        std::vector< int64_t > sizecount;
        std::vector< int64_t > sizebytes;

        if ( !RUNNING_ON_VALGRIND && !getenv( "DIVINE_POOL_STATS" ) )
            return ;

        for ( int i = 0; i < blockcount; ++i )
            if ( _s->vhandles[ i ] ) {
                for ( int j = 0; j < header( i ).total; ++j ) {
                    bool allocd = _s->vhandles[ i ][ j ].allocated;
                    count += allocd;
                    bump( sizecount, header( i ).itemsize );
                    sizecount[ header( i ).itemsize ] += allocd;
                }
                delete[] _s->vhandles[ i ].load();
            }

        for ( int i = 0; i < blockcount; ++i )
            if ( _s->block[ i ] )
            {
                int64_t is = header( i ).itemsize;
                int64_t b = header( i ).total * align( is, sizeof( Pointer ) );
                bytes += b;
                bump( sizebytes, is + 1 );
                sizebytes[ is ] += b;
                VALGRIND_DESTROY_MEMPOOL( _s->block[ i ] );
            }

        bump( sizecount, sizebytes.size() - 1 );
        bump( sizebytes, sizecount.size() - 1 );

        std::cerr << "~Pool(): " << count << " objects not freed:" << std::endl;
        for ( size_t i = 0; i < sizecount.size(); ++ i )
            if ( sizecount[i] || sizebytes[ i ] ) {
                int64_t c = sizecount[i];
                int64_t b = c * i;
                int64_t t = sizebytes[i];
                wasted += (t - b);
                std::cerr << "   " << std::setw(8) << c << " object(s) of size " << i
                          << " for " << b / 1024 << "/" << t / 1024 << "kB" << std::endl;
            }
        std::cerr << " " << (bytes / 1024) << " kbytes held; " << wasted / 1024 << "kB wasted" << std::endl;
    }

#pragma GCC diagnostic pop
#else

#define VALGRIND_MAKE_MEM_DEFINED(x, y)
#define VALGRIND_MAKE_MEM_NOACCESS(x, y)
#define VALGRIND_MAKE_MEM_UNDEFINED(x, y)

    void valgrindAllocated( Pointer, bool ) {}
    void valgrindDeallocated( Pointer ) {}
    void valgrindNewBlock( int, int ) {}
    void valgrindFini() {}
    void valgrindInit() {}

#endif

    /*
     * NB. We set usedblocks to 8, so that we both keep reasonable alignment
     * and make (0, 0) Pointer invalid; this may change in the future, when
     * Extensions, which tend to contain Pointers, are no longer zeroed, but
     * constructed instead (as they should)
     */
    Pool() : _s( new Shared() )
    {
        _s->usedblocks = 8;
        for ( int i = 0; i < 4096; ++i )
            _s->freelist[ i ] = nullptr;
        for ( int i = 0; i < 4096; ++i )
            _s->freelist_big[ i ] = nullptr;
        for ( int i = 0; i < blockcount; ++i )
            _s->block[ i ] = nullptr;
        valgrindInit();
        initL();
    }

    ~Pool()
    {
        for ( int i = 0; i < 4096; ++i )
            delete[] _l.size_big[ i ];

        if ( _s.use_count() > 1 )
            return;

        /* concurrent copying and destruction of the same object is not
           allowed */
        valgrindFini();
        for ( int i = 0; i < 4096; ++i ) {
            nukeList( _s->freelist[ i ] );
            if ( _s->freelist_big[ i ] ) {
                for ( int j = 0; j < 4096; ++j )
                    nukeList( _s->freelist_big[ i ][ j ] );
                delete[] _s->freelist_big[ i ].load();
            }
        }
        for ( int i = 0; i < blockcount; ++i ) {
            if ( !_s->block[ i ] )
                continue;
            auto size =
                header( i ).total ?
                header( i ).total * align( header( i ).itemsize,
                                           sizeof( Pointer ) ) +
                sizeof( BlockHeader ) : blocksize;
            brick::mmap::MMap::drop( _s->block[ i ], size );
        }
    }

    Pool( const Pool &o ) : _s( o._s ) { initL(); }

    void initL()
    {
        for ( int i = 0; i < 4096; ++i )
            _l.size_big[ i ] = nullptr;
        _l.size[ 0 ].blocksize = blocksize;
        // _l.ephemeral_block = newblock( 0 );
        // header( _l.ephemeral_block ).itemsize = 4;
    }

    std::atomic< FreeList * > &freelist( int size )
    {
        if ( size < 4096 )
            return _s->freelist[ size ];

        std::atomic< FreeList * > *chunk, *newchunk;
        if ( !( chunk = _s->freelist_big[ size / 4096 ] ) ) {
            if ( _s->freelist_big[ size / 4096 ].compare_exchange_strong(
                     chunk, newchunk = new FreeListPtr[ 4096 ]() ) )
                chunk = newchunk;
            else
                delete newchunk;
        }
        ASSERT( chunk );
        return chunk[ size % 4096 ];
    }

    int &ephemeralSize( Pointer p )
    {
        return *reinterpret_cast< int * >( dereference( p ) - sizeof( int ) );
    }

    Pointer ephemeralAllocate( int sz )
    {
        /* TODO valgrind */
        ASSERT_LEQ( 0, _l.ephemeral_block );
        ASSERT_LEQ( _l.ephemeral_offset + sz, blocksize );
        Pointer p( _l.ephemeral_block, _l.ephemeral_offset + sizeof( int ) );
        _l.ephemeral_offset += align( sz, 4 ) + sizeof( int );
        ephemeralSize( p ) = sz;
        return p;
    }

    void ephemeralClear()
    {
        /* TODO valgrind */
        _l.ephemeral_offset = 0;
    }

    bool valid( Pointer p ) { return p.slab(); }

    template< typename T >
    T *machinePointer( Pointer p, int off = 0 )
    {
        return reinterpret_cast< T * >( dereference( p ) + off );
    }

    int size( Pointer p )
    {
        if( int( p.slab() ) == _l.ephemeral_block )
        {
            ASSERT_LEQ( p.chunk().get(), _l.ephemeral_offset );
            return ephemeralSize( p );
        }

        ASSERT( header( p ).total > 0 && "invalid size() call on a foreign ephemeral block" );
        ASSERT( header( p ).itemsize );
        return header( p ).itemsize;
    }

    Pointer &freechunk( Pointer p )
    {
        return *reinterpret_cast< Pointer * >( dereference( p ) );
    }

    Pointer fromFreelist( SizeInfo &si )
    {
        ASSERT( si.touse.count );
        ASSERT( valid( si.touse.head ) );
        -- si.touse.count;
        Pointer p = si.touse.head;
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wold-style-cast"
        VALGRIND_MAKE_MEM_DEFINED( dereference( p ), sizeof( Pointer ) );
        si.touse.head = freechunk( p );
        VALGRIND_MAKE_MEM_NOACCESS( dereference( p ), sizeof( Pointer ) );
#pragma GCC diagnostic pop
        return p;
    }

    Pointer allocate( int size )
    {
        Pointer p;

        auto &si = sizeinfo( size );
        /* try our private freelist first */

        if ( !si.touse.count && si.tofree.count ) {
            si.touse = si.tofree;
            si.tofree = FreeList();
        }

        if ( si.touse.count )
            p = fromFreelist( si );
        else { /* nope. try a partially filled block */
            if ( si.active >= 0 && usable( si.active ) ) {
                p.slab() = si.active;
                p.chunk() = header( p ).allocated ++;
            } else { /* still nothing. try nicking something from the shared freelist */
                std::atomic< FreeList * > &fhead = freelist( size );
                FreeList *fb = fhead;
                while ( fb && !fhead.compare_exchange_weak( fb, fb->next ) );
                if ( fb ) {
                    si.touse = *fb;
                    si.touse.next = nullptr;
                    delete fb;
                    p = fromFreelist( si );
                } else { /* give up and allocate a fresh block */
                    p.slab() = newblock( size );
                    p.chunk() = header( p ).allocated ++;
                }
            }
        }

        valgrindAllocated( p, false );
        return p;
    }

    template< typename Origin >
    void materialise( Pointer p, int size, Origin &origin )
    {
        int b = p.slab();
        if ( !_s->block[ b ] )
        {
            const int overhead = sizeof( BlockHeader );
            const int allocsize = align( size, sizeof( Pointer ) );
            const int total = origin.header( p ).total;
            const int allocate = overhead + total * allocsize;
            auto mem = brick::mmap::MMap::alloc( allocate );
            _s->block[ b ] = static_cast< BlockHeader * >( mem );
            header( b ).itemsize = size;
            header( b ).total = total;
            header( b ).allocated = 0;
            valgrindNewBlock( b, total );
        }
        valgrindAllocated( p, true );
    }

    void free( Pointer p )
    {
        if ( !valid( p ) )
            return;

        ASSERT( header( p ).total > 0 && "trying to free ephemeral block" );

        auto &si = sizeinfo( size( p ) );
        FreeList *fl = si.touse.count < 4096 ? &si.touse : &si.tofree;
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wold-style-cast"
        VALGRIND_MAKE_MEM_UNDEFINED( dereference( p ), sizeof( Pointer ) );
#pragma GCC diagnostic pop
        freechunk( p ) = fl->head;
        fl->head = p;
        ++ fl->count;

        valgrindDeallocated( p );

        /* if there's a lot on our freelists, give some to the pool */
        if ( fl == &si.tofree && fl->count >= 4096 ) {
            std::atomic< FreeList * > &fhead = freelist( size( p ) );
            fl = new FreeList( *fl );
            fl->next = fhead;
            while ( !fhead.compare_exchange_weak( fl->next, fl ) );
            si.tofree = FreeList();
        }
    }

    bool usable( int b )
    {
        return _s->block[ b ] && header( b ).allocated < header( b ).total;
    }

    SizeInfo &sizeinfo( int index )
    {
        if ( index < 4096 )
            return _l.size[ index ];
        if ( !_l.size_big[ index / 4096 ] )
            _l.size_big[ index / 4096 ] = new SizeInfo[ 4096 ];
        return _l.size_big[ index / 4096 ][ index % 4096 ];
    }

    int newblock( int size )
    {
        int b = 0;

        if ( _l.emptyblocks.empty() ) {
            b = _s->usedblocks.fetch_add( 16 );
            for ( int i = b + 1; i < b + 16; ++i )
                _l.emptyblocks.push_back( i );
        } else {
            b = _l.emptyblocks.back();
            _l.emptyblocks.pop_back();
        }

        auto &si = sizeinfo( size );

        const int overhead = sizeof( BlockHeader );
        const int allocsize = align( size, sizeof( Pointer ) );
        si.blocksize = std::max( allocsize + overhead, si.blocksize );
        const int total = allocsize ? ( si.blocksize - overhead ) / allocsize : 0;
        const int allocate = allocsize ? overhead + total * allocsize : blocksize;

        auto mem = brick::mmap::MMap::alloc( allocate );
        _s->block[ b ] = static_cast< BlockHeader * >( mem );
        header( b ).itemsize = size;
        header( b ).total = total;
        header( b ).allocated = 0;
        valgrindNewBlock( b, total );
        si.blocksize = std::min( 4 * si.blocksize, int( blocksize ) );
        return si.active = b;
    }

    BlockHeader &header( Pointer p ) { return header( p.slab() ); }
    BlockHeader &header( int b )
    {
        ASSERT( _s );
        ASSERT_LEQ( b, blockcount );
        ASSERT_LEQ( 0, b );
        ASSERT( _s->block[ b ] );
        return *(_s->block[ b ]);
    }

    char *dereference( Pointer p )
    {
        auto &h = header( p );
        return h.data + p.chunk() * align( h.itemsize, sizeof( Pointer ) );
    }

};

template< typename P >
static inline auto operator<<( std::ostream &o, P p ) -> decltype( p.slab(), o )
{
    return o << p.slab().get() << ":" << p.chunk().get() << " " << p.tag().get();
}

}

namespace t_mem {

template< int _a, int _b, int _c >
struct Pool
{
    using _Pool = mem::Pool< 20, 19, uint64_t, _a, _b, _c >;

    struct Checker : shmem::Thread
    {
        char padding[128];
        _Pool m_pool;
        std::deque< typename _Pool::Pointer > ptrs;
        int limit;
        unsigned seedp;
        int terminate;
        char padding2[128];

        _Pool &pool() { return m_pool; }

        bool decide( int i )
        {
            int j = rand() % limit;
            if ( i + j > limit )
                return false;
            return true;
        }

        void main()
        {
            limit = 32*1024;
            int state = 0;
            for ( int i = 0; i < limit; ++i ) {
                ASSERT( state >= 0 );
                if ( decide( i ) || ptrs.empty() ) {
                    ++ state;
                    ptrs.push_back( pool().allocate( 32 ) );
                } else {
                    -- state;
                    pool().free( ptrs.front() );
                    ptrs.pop_front();
                }
            }
            while ( !ptrs.empty() ) {
                pool().free( ptrs.front() );
                ptrs.pop_front();
            }
        }

        Checker()
            : terminate( 0 ) {}
    };

    TEST(align)
    {
        ASSERT_EQ( mem::align( 2, 4 ), 4 );
        ASSERT_EQ( mem::align( 3, 4 ), 4 );
        ASSERT_EQ( mem::align( 5, 4 ), 8 );
        ASSERT_EQ( mem::align( 0, 4 ), 0 );
    }

    TEST(sequential)
    {
        Checker c;
        c.main();
    }

    TEST(materialise)
    {
        _Pool a, b;
        typename _Pool::Pointer p[100];
        for ( int i = 0; i < 100; ++i )
        {
            p[i] = a.allocate( 8 );
            *a.template machinePointer< int >( p[i] ) = i;
            b.materialise( p[i], 4, a );
            *b.template machinePointer< int >( p[i] ) = i;
        }
        for ( int i = 0; i < 100; ++i )
        {
            ASSERT_EQ( *a.template machinePointer< int >( p[i] ), i );
            ASSERT_EQ( *b.template machinePointer< int >( p[i] ), i ) ;
        }
    }

    TEST(parallel)
    {
        std::vector< Checker > c;
        Checker x;
        for ( int i = 0; i < 3; ++i )
            c.emplace_back( x );
        // c.resize( 3, Checker() );
        for ( int j = 0; j < 5; ++j ) {
            for ( auto &t : c ) t.start();
            for ( auto &t : c ) t.join();
        }
    }
};

template struct Pool< 2, 1, 0 >;
template struct Pool< 1, 2, 0 >;
template struct Pool< 0, 1, 2 >;

}

}

// vim: syntax=cpp tabstop=4 shiftwidth=4 expandtab
